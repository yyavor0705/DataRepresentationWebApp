import io
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import logging

N_SAMPLE = int(1e5)  # Declear first, as it provides global default value for helper functions

# logger
# log levels: NOTSET, DEBUG, INFO, WARNING, ERROR, and CRITICAL
LOG_FORMAT = "%(levelname)s %(asctime)s - %(message)s"
logging.basicConfig(filename='mylog.log',
                    # level=logging.DEBUG,
                    format=LOG_FORMAT)

logger = logging.getLogger(__name__)
logger.setLevel(
    logging.CRITICAL)  # set logging level here to work in jupyter notebook where maybe a default setting


# master helper functions

def read_input_file(filename):
    # from file
    df = pd.read_excel(filename)
    return df


def read_input_user(prompt):
    val = float(input(prompt))
    return val


# Helper function
def Get_mean(x):
    """get mean ignoring nans"""
    x = x[~np.isnan(x)]  # remove nans
    return x.mean()


def Get_std(x):
    """get standard deviation ignoring nans"""
    x = x[~np.isnan(x)]  # remove nans
    return x.std()


def Hist_custom(S):
    """plot histgram with N_SAMPLE//100 bins ignoring nans"""
    S_dropna = S[~np.isnan(S)]
    fig, ax = plt.subplots()
    ax.hist(S_dropna, bins=min(N_SAMPLE // 100, 100), density=True, alpha=0.5, color='C0')


# Sampler updated
def Normal_custom(m, s, n_sample=N_SAMPLE, non_negative=False, plot=False):
    """ Sampling from a normal distribution

    Parameters
    ----------
    m : int or float
        mean
    s : int or float
        standard deviation
    n_sample : int
        sample number, default is a Global var N_SAMPLE
    non_negative: bool
        if true, return trangcational distributiong with no negatives, defaut is False
    plot : bool
        default is False

    Returns
    -------
    out : numpy array
    """
    x = np.random.normal(loc=m, scale=s, size=n_sample)
    if non_negative:
        x = stats.truncnorm.rvs((0 - m) / s, (np.inf - m) / s, loc=m, scale=s, size=n_sample)
    if plot:
        fig, ax = plt.subplots()
        ax.hist(x)
    return x


def Beta_custom(m, s, a, b, n_sample=N_SAMPLE, plot=False):
    """ draw samples from a general beta distribution discribed by mean, std and upper and lower bounds
    X~General Beta(a,b, loc = c, scale = d)
    Z~std Beta(alpha, beta)

    X = c + d*Z
    E(X) = c + d * E(Z)
    var(X) = d^2 * var(Z)

    Parameters
    ----------
    m : mean
    s : standard deviation
    a : upper bound, not shape param a(alpha)
    b : lower bound, not shape param b(beta)
    n_sample: int
        sample number
    plot : bool
        default is False

    Reterns
    -------
    out : numpy array
    """
    # location:c and scale:d for General Beta (standard Beta range [0,1])
    c = a
    d = b - a

    # mean and varance for
    mu = (m - c) / d
    var = s ** 2 / d ** 2

    # shape params for Z~standard beta
    alpha = ((1 - mu) / var - 1 / mu) * mu ** 2
    beta = alpha * (1 / mu - 1)
    z = np.random.beta(alpha, beta, size=n_sample)

    # transfer back to General Beta
    x = c + d * z

    if plot:
        fig, ax = plt.subplots()
        ax.hist(x)
        print(x.mean(), x.std())
    return x


def interp_extrap_f(x, y, x_find, plot=False):
    """interpolate or extrapolate value from an array with fitted2-deg or 3-deg polynomial

    Parameters
    ----------
    x : array-like
        varible
    y : array-like
        function value
    x_find : int or float or array-like
        look-up x
    plot : bool
        plot curve fit and data points, default if false

    Returns
    -------
    int or float or array-like
        inter/extraplolated value(s), raise warning when extrapolation is used
    """

    def func2(x, a, b, c):
        # 2-order polynomial
        return a * (x ** 2) + b * (x ** 1) + c

    def func3(x, a, b, c, d):
        # 3-order polynomial
        return a * (x ** 2) + b * (x ** 2) + c * x + d

    if x_find < x.min() or x_find > x.max():
        logger.warning("Warning: extrapolation used")

    from scipy.optimize import curve_fit
    # Initial parameter guess, just to kick off the optimization
    if len(y) > 3:
        logger.debug('use func3: 3-deg polynomial')
        guess = (0.5, 0.5, 0.5, 0.5)
        popt, _ = curve_fit(func3, x, y, p0=guess)
        y_find = func3(x_find, *popt)
        if plot:
            fig, ax = plt.subplots()
            ax.plot(x, y, '.', label='table')
            _plot_data = np.linspace(x.min(), x.max(), 100)
            ax.plot(_plot_data, func3(_plot_data, *popt), '--')
            ax.plot(x_find, y_find, 'x', color='r', markersize=8, label='interp/extrap data')
            ax.legend()

    if len(y) <= 3:
        logger.debug('use func2: 2-deg polynomial')
        guess = (0.5, 0.5, 0.5)
        popt, _ = curve_fit(func2, x, y, p0=guess)
        y_find = func2(x_find, *popt)
        if plot:
            fig, ax = plt.subplots()
            ax.plot(x, y, '.', label='table')
            _plot_data = np.linspace(x.min(), x.max(), 100)
            ax.plot(_plot_data, func2(_plot_data, *popt), '--')
            ax.plot(x_find, y_find, 'x', color='r', markersize=8, label='interp/extrap data')
            ax.legend()
    return y_find


def find_similar_group(item_list, similar_group_size=2):
    """find similar sublist of similar_group_size from a item_list"""
    from itertools import combinations
    combos = np.array(list(combinations(item_list, similar_group_size)))
    ind_min = combos.std(axis=1).argmin()
    similar_group = (combos[ind_min].tolist())
    return similar_group


# helper function
def Fit_distrib(s, fit_type='kernel', plot=False, xlabel='', title='', axn=None):
    """fit data to a probablility distribution function(parametric or numerical)
    and return a continuous random varible or a random varible represented by Gaussian kernels
    parametric : normal
    numerical : Gaussian kernels

    Parameters
    ----------
    s : array-like
        sample data
    fit_type : string
        fit type keywords, 'kernel', 'normal'
    plot : bool
        create a plot with hsitgram and fitted pdf curve
    **kwargs : plot control

    Reterns
    -------
    out : continuous random varible : stats.norm(loc = mu, scale = sigma)
              when parametric normal is used
          Gaussian kernel random varible : (stats.gaussian_kde)
              when kernel is used
    """
    mu = None
    sigma = None
    kde = None
    if fit_type == 'normal':
        # parametric, fit normal distribution
        logger.debug('parametric, fit normal distribution')

        mu, sigma = stats.norm.fit(s, floc=s.mean())  # Fit a curve to the variates  mu is loc sigma is scale

    if fit_type == 'kernel':
        # non-parametric, this creates the kernel, given an array it will estimate the probability over that values
        logger.debug('non-parametric kernel fit')
        s_dropna = s[~np.isnan(s)]  # remove nans
        kde = stats.gaussian_kde(
            s_dropna)  # bandwidth selection:  gaussian_kde uses a rule of thumb, the default is Scottâ€™s Rule.

    if plot:
        if axn is None:
            axn = plt.gca()
        n = min(len(s) // 100, 100)  # bin size
        dist_space = np.linspace(min(s), max(s), 100)
        axn.hist(s, bins=n, density=True)

        # plot pdf
        if fit_type == 'normal':
            pdf = stats.norm.pdf(dist_space, mu, sigma)  # probability distribution
            axn.plot(dist_space, pdf, label='normal')

        if fit_type == 'kernel':
            pdf_kde = kde(dist_space)
            axn.plot(dist_space, pdf_kde, label='kernel')

        axn.set_xlabel(xlabel)
        axn.set_ylabel('distribution density')
        axn.legend(loc='upper right')
        axn.set_title(title)
    # return
    if fit_type == 'normal':
        return stats.norm(loc=mu, scale=sigma)
    if fit_type == 'kernel':
        return kde

    # Generic Postproc: Calculate probability of failure


def Pf_RS(R_info, S, R_distrib_type='normal', plot=False):  # updated!
    """Calculate the probability of failure  Pf = P(R-S<0), given the R(resistance) and S(load)
       with three three menthods and use method 3) if checked OK with the other two
           1) crude monte carlo
           2) numerical ingeral of g kenel fit
           3) R S integral: ('$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$')
       reliability index(beta factor) is calculated with simple 1st order g.mean()/g.std()

    Parameters
    ----------
    R_info : tuple or numpy array
             distribution of Resistance, e.g. cover thickness, critical chloride content, tensile strength
             can be array or distribution parameters
             R_distrib_type='normal' -> tuple(m,s) for normal m: mean s: standard deviation
             R_distrib_type='normal' -> tuple(m,s,a,b) for (General) beta distribution m: mean, s: standard deviation a,b : upper, lower bound
             R_distrib_type='normal' -> array: for not-determined distribution, will be treated numerically(R S integral is not applied )

    S : numpy array
        distribution of load, e.g. carbonation depth, chlride content, tensile stress
        the distrubtion type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel

    R_distrib_type : string
        'normal', 'beta', 'array'

    Returns
    -------
    out = tuple
        (probability of failure, reliability index)

    Notes
    -----
    For R as arrays R S integral is not applied
    R S integralation method: $P_f = P(R-S<=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$
    the dual numerical integration seems too computationally expensive, so consider fit R to analytical distribution in the future versions

    """
    from scipy import integrate
    R, pf_RS = (None, None)

    S_kde_fit = Fit_distrib(S, fit_type='kernel')
    S_dropna = S[~np.isnan(S)]
    try:
        if R_distrib_type == 'normal':
            # R = (mu, std)
            (m, s) = R_info
            R_distrib = stats.norm(m, s)
            R = R_distrib.rvs(size=N_SAMPLE)

            # Calculate probablility of failure
            #     $P_f = P(R-S<=0)=\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$
            pf_RS = integrate.quad(lambda x: R_distrib.cdf(x) * S_kde_fit(x)[0],
                                   0, S_dropna.max())[0]

        if R_distrib_type == 'beta':
            # R = (m, s, a, b) a, b are lower and upper bound
            (m, s, a, b) = R_info

            # location:c and scale:d for General Beta (standard Beta range [0,1])
            # calculate loc and scale
            c = a
            d = b - a

            # mean and variance for
            mu = (m - c) / d
            var = s ** 2 / d ** 2

            # shape params for Z~standard beta
            alpha = ((1 - mu) / var - 1 / mu) * mu ** 2
            beta = alpha * (1 / mu - 1)

            R_distrib = stats.beta(alpha, beta, c, d)
            R = R_distrib.rvs(size=N_SAMPLE)

            # Calculate probablility of failure
            #     $P_f = P(R-S<=0)=\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$
            pf_RS = integrate.quad(lambda x: R_distrib.cdf(x) * S_kde_fit(x)[0],
                                   0, S_dropna.max())[0]

        if R_distrib_type == 'array':
            # dual numerical integration is too expensive, consider fit R to analytical distribution!!!!!!!!!!!!!!!!!!!!!!!!!!
            # plot condition to be fixed !!!!!!!!!!!!!!!!!!!!!!

            #         # use R array
            #         R_kde_fit = Fit_distrib(R, fit_type='kernel')
            #         R_dropna = R[~np.isnan(R)]
            #         # $P_f = P(R-S<=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$

            #         def R_cdf_S_pdf(x, R_kde_fit, S_kde_fit):
            #             R_cdf = integrate.quad(lambda z: R_kde_fit(z)[0],0,x)[0] # kde_fit returns ([array needed]). therefore use lamda z kde(z)[0]
            #             S_pdf = S_kde_fit(x)[0]
            #             return R_cdf*S_pdf

            #         pf_RS = integrate.quad(R_cdf_S_pdf,0,S_dropna.max(), args=(R_kde_fit, S_kde_fit))[0]
            pf_RS = None
    except:
        print('R is not configured')

    # compare with
    # numerical g
    g = R - S
    g = g[~np.isnan(g)]
    # numerical kernel fit
    g_kde_fit = Fit_distrib(g, fit_type='kernel', plot=False)

    pf_kde = integrate.quad(g_kde_fit, g.min(), 0)[0]
    pf_sample = len(g[g < 0]) / len(g)
    beta_factor = g.mean() / g.std()  # first order

    # check for tiny tail
    if pf_sample < 1e-10:
        print("warning: very small Pf ")
        logger.warning("warning: very small Pf ")

    # check if pf_RS is the pf (should be)
    best_2_of_3 = find_similar_group([pf_sample, pf_kde, pf_RS], similar_group_size=2)
    if pf_RS not in best_2_of_3:
        logger.warning("warning: pf_RS is not used, double check")
        logger.warning(
            'Pf(g = R-S < 0) from various methods\n    sample count: {}\n    g integral: {}\n    R S integral: {}\n    beta_factor: {}'.format(
                pf_sample, pf_kde, pf_RS, beta_factor))

    logger.info(
        'Pf(g = R-S < 0) from various methods\n    sample count: {}\n    g integral: {}\n    R S integral: {}\n    beta_factor: {}'.format(
            pf_sample, pf_kde, pf_RS, beta_factor))

    plot_image_buf = io.BytesIO()
    text_lines_to_report = list()
    if plot:
        text_lines_to_report = (
            f"Pf(g = R-S < 0) from various methods:",
            f"      sample count: {pf_sample}",
            f"      integral: {pf_kde}",
            f"      R S integral: {pf_RS}",
            f"      beta_factor: {beta_factor}"
        )
        print('Pf(g = R-S < 0) from various methods')
        print('    sample count: {}'.format(pf_sample))
        print('    g integral: {}'.format(pf_kde))
        print('    R S integral: {}'.format(pf_RS))
        # printmd('$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$')
        print('    beta_factor: {}'.format(beta_factor))

        # Plot R S
        fig, [ax1, ax2] = plt.subplots(ncols=2, figsize=(10, 3))
        # R
        R_plot = np.linspace(R.min(), R.max(), 100)
        ax1.plot(R_plot, R_distrib.pdf(R_plot), color='C0')
        ax1.hist(R, bins=min(N_SAMPLE // 100, 100), density=True, alpha=0.5, color='C0', label='R')

        # S
        S_plot = np.linspace(S_dropna.min(), S_dropna.max(), 100)
        ax1.plot(S_plot, S_kde_fit(S_plot), color='C1', alpha=1)
        ax1.hist(S_dropna, bins=min(N_SAMPLE // 100, 100), density=True, alpha=0.5, color='C1', label='S')

        ax1.set_title('S: mean = {:.1f} stdev = {:.1f}'.format(S_dropna.mean(), S_dropna.std()))
        ax1.legend()
        plt.tight_layout()

        # plot g
        g_plot = np.linspace(g.min(), g.max(), 100)
        ax2.plot(g_plot, g_kde_fit(g_plot), color='C2', alpha=1)

        ax2.hist(g, density=True, bins=min(N_SAMPLE // 100, 100), color='C2', alpha=0.5, label='g=R-S')
        ax2.vlines(x=0, ymin=0, ymax=g_kde_fit(0)[0], linestyles='--', alpha=0.5)
        ax2.vlines(x=g.mean(), ymin=0, ymax=g_kde_fit(g.mean())[0], linestyles='--', alpha=0.5)
        #         ax.annotate(s='', xy=(0,g_kde_fit(0)[0]), xytext=(g.mean(),g_kde_fit(0)[0]),
        #                     arrowprops={'arrowstyle': '<->'},va='center')
        ax2.annotate(s=r'$\{mu}_g$', xy=(0, g.mean()), xytext=(g.mean(), g_kde_fit(0)[0]),
                     va='center')
        ax2.legend()
        ax2.set_title('Limit-state P(g<0)={}'.format(pf_RS))

        plt.savefig(plot_image_buf, format='png')
        plot_image_buf.seek(0)
        plt.show()

    return (pf_RS, beta_factor, R_distrib, S_kde_fit), (plot_image_buf, text_lines_to_report)


def RS_plot(model, ax=None, t_offset=0, amplify=1):  # updated!
    """plot R S distribution vertically at a time to an axis

    Parameters
    ----------
    model.R_distrib : scipy.stats._continuous_distns, normal or beta
                      calculated in Pf_RS() through model.postproc()
    model.S_kde_fit : stats.gaussian_kde
                      calculated in Pf_RS() through model.postproc()
                      distribution of load, e.g. carbonation depth, chlride content, tensile     stress. The distrubtion type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel

    model.S : numpy array
              load, e.g. carbonation depth, chlride content, tensile stress
    ax : axis
    t_offset : time offset to move the plot along the t-axis. default is zero
    amplify : scale the height of the pdf plot
    """

    R_distrib = model.R_distrib
    S_kde_fit = model.S_kde_fit
    S = model.S

    S_dropna = S[~np.isnan(S)]
    # Plot R S
    R = R_distrib.rvs(size=N_SAMPLE)

    if ax == None:
        ax = plt.gca()
    # R
    R_plot = np.linspace(R.min(), R.max(), 100)
    ax.plot(R_distrib.pdf(R_plot) * amplify + t_offset, R_plot, color='C0')
    ax.fill_betweenx(R_plot, t_offset, R_distrib.pdf(R_plot) * amplify + t_offset, color='C0', alpha=0.5, label='R')
    # S
    S_plot = np.linspace(S_dropna.min(), S_dropna.max(), 100)
    ax.plot(S_kde_fit(S_plot) * amplify + t_offset, S_plot, color='C1', alpha=1)
    ax.fill_betweenx(S_plot, t_offset, S_kde_fit(S_plot) * amplify + t_offset, color='C1', alpha=0.5, label='S')
